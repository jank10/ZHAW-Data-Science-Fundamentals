{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression & Gradient Descent\n",
    "\n",
    "In this notebook we will work with the diabetes dataset \n",
    "https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\n",
    "https://web.stanford.edu/~hastie/Papers/LARS/diabetes.data (raw data, not standardised)\n",
    "\n",
    "The data set contains 10 baseline variables (X1, X2, X3....X10) for 442 patients with diabetes: age, sex, body mass index, average blood pressure, and six blood serum measurements. The target variable (y) indicates the progression of the dieases one year after baseline.\n",
    "\n",
    "Luckily, this database is available in sklearn, so we need only to load it. \n",
    "Even more luckily, the sklearn database is already standardised (compare with the raw data above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_X, diabetes_y = datasets.load_diabetes(return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will consider a subset of the dataset, specifically only the variables: body mass index, average blood pressure and one blood serum measurement. Feel free to include more dimensions in your model. \n",
    "As usual insert your code in \"####ADD YOUR CODE HERE#####\" and answer the questions along the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_X = diabetes_X[:,2:5] #select only the 3 relevant features we want to consider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing, we split our dataset in training set and test set (z.B 80 data points are left as a test).\n",
    "We will fit(train) our model on the training dataset and then we will evaluate how they really perform on test data.\n",
    "The performance on the test data is fundamental to see how models generalise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_X_train = diabetes_X[:-80] #feel free to change 80 to another value, you can also split data using the sklearn function: train_test_split\n",
    "diabetes_X_test = diabetes_X[-80:]\n",
    "\n",
    "diabetes_y_train = diabetes_y[:-80]\n",
    "diabetes_y_test = diabetes_y[-80:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Notebook we want to first compare several methods to determine the best parameters for our model. \n",
    "Let's start with the LinearRegression() function from sklearn (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html).\n",
    "The LinearRegression class is based on the scipy.linalg.lstsq() function (the name stands for \"least squares\"), this compute the optimal paramters'values using the least square exact method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients (theta_1, theta_2, theta_3, theta_0): \n",
      "\n",
      "805.692327767192 363.7020387241161 52.144503639311466 152.53452637832743\n"
     ]
    }
   ],
   "source": [
    "#regr = linear_model.LinearRegression(fit_intercept=True, copy_X=True, n_jobs=None, positive=False)\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "#Once you defined your model, use the method .fit to fit to the data \n",
    "regr.fit(diabetes_X_train, diabetes_y_train)\n",
    "\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients (theta_1, theta_2, theta_3, theta_0): \\n')\n",
    "print(regr.coef_[0], regr.coef_[1], regr.coef_[2], regr.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now try to compute the same parameters using the Batch Gradient Descent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.1  # learning rate, feel free to change\n",
    "n_iterations = 100000\n",
    "N = len(diabetes_X)\n",
    "\n",
    "theta_0 = np.random.randn(1) # random initialization\n",
    "theta_1 = np.random.randn(1)\n",
    "theta_2 = np.random.randn(1)\n",
    "theta_3 = np.random.randn(1)\n",
    "\n",
    "X1= diabetes_X_train[:,0]\n",
    "X2= diabetes_X_train[:,1]\n",
    "X3= diabetes_X_train[:,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code here the formulas for the batch gradient descent (check the lecture slides for reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iteration in range(n_iterations):\n",
    "\n",
    "    \n",
    "    gradient_theta_3 = 1/N * np.sum((theta_0 + theta_1 * X1 + theta_2 * X2 + theta_3 * X3 - diabetes_y_train) * X3)\n",
    "    gradient_theta_2 = 1/N * np.sum((theta_0 + theta_1 * X1 + theta_2 * X2 + theta_3 * X3 - diabetes_y_train) * X2)\n",
    "    gradient_theta_1 = 1/N * np.sum((theta_0 + theta_1 * X1 + theta_2 * X2 + theta_3 * X3 - diabetes_y_train) * X1)\n",
    "    gradient_theta_0 = 1/N * np.sum((theta_0 + theta_1 * X1 + theta_2 * X2 + theta_3 * X3 - diabetes_y_train))\n",
    "    \n",
    "    \n",
    "    theta_3 = theta_3 - eta * gradient_theta_3\n",
    "    theta_2 = theta_2 - eta * gradient_theta_2                             \n",
    "    theta_1 = theta_1 - eta * gradient_theta_1\n",
    "    theta_0 = theta_0 - eta * gradient_theta_0\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[805.68579052] [363.70741505] [52.14565847] [152.53451877]\n"
     ]
    }
   ],
   "source": [
    "#Let's see the parameters look like \n",
    "print(theta_1,theta_2,theta_3, theta_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (3,) (80,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/workspaces/ZHAW-Data-Science-Fundamentals/Week10/Exercise_LinearReg_Gradient.ipynb Cell 16\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Bglorious-acorn-qr77pqrp5g7h445j/workspaces/ZHAW-Data-Science-Fundamentals/Week10/Exercise_LinearReg_Gradient.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#Value of the cost function\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Bglorious-acorn-qr77pqrp5g7h445j/workspaces/ZHAW-Data-Science-Fundamentals/Week10/Exercise_LinearReg_Gradient.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# Linear regression\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://codespaces%2Bglorious-acorn-qr77pqrp5g7h445j/workspaces/ZHAW-Data-Science-Fundamentals/Week10/Exercise_LinearReg_Gradient.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m costfunc_reg \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39mlen\u001b[39m(diabetes_X) \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39msum(regr\u001b[39m.\u001b[39;49mintercept_ \u001b[39m+\u001b[39;49m regr\u001b[39m.\u001b[39;49mcoef_[\u001b[39m0\u001b[39;49m]\u001b[39m*\u001b[39;49mdiabetes_X_test[\u001b[39m0\u001b[39;49m] \u001b[39m+\u001b[39;49m regr\u001b[39m.\u001b[39;49mcoef_[\u001b[39m1\u001b[39;49m]\u001b[39m*\u001b[39;49mdiabetes_X_test[\u001b[39m1\u001b[39;49m] \u001b[39m+\u001b[39;49m regr\u001b[39m.\u001b[39;49mcoef_[\u001b[39m2\u001b[39;49m]\u001b[39m*\u001b[39;49mdiabetes_X_test[\u001b[39m2\u001b[39;49m] \u001b[39m-\u001b[39;49m diabetes_y_test)\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (3,) (80,) "
     ]
    }
   ],
   "source": [
    "#Value of the cost function\n",
    "# Linear regression\n",
    "\n",
    "#tip from prof: check inline comprehensive list instead of using loop\n",
    "\n",
    "costfunc_reg = 1/2*len(diabetes_X) * np.sum(regr.intercept_ + regr.coef_[0]*diabetes_X_test[0] + regr.coef_[1]*diabetes_X_test[1] + regr.coef_[2]*diabetes_X_test[2] - diabetes_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare with the values obtained with the function LinearRegression().\n",
    "Try to change the number of iterations and/or eta to get closer to those values.\n",
    "\n",
    "Actually the best thing would be not to compare the parameters itself but the value of the cost function!\n",
    "Optional: Try to compute in both cases the cost function value. How do they compare?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent\n",
    "We try now to see how the results change if we use the SGD approach. We use here the sklearn function SGDRegressor.\n",
    "Check out here the syntax\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html\n",
    "This function has many parameters, for the moment just focus on the number of iterations and eta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SGDRegressor(max_iter=1000000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SGDRegressor</label><div class=\"sk-toggleable__content\"><pre>SGDRegressor(max_iter=1000000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SGDRegressor(max_iter=1000000)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_reg = linear_model.SGDRegressor(max_iter= 1000000, eta0=0.01)\n",
    "sgd_reg.fit(diabetes_X_train, diabetes_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([643.13219589, 372.14993907, 119.39078242]), array([152.35705811]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_reg.coef_, sgd_reg.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does this compare with the previous solutions? Try to adapt also here eta and number of iterations. \n",
    "How does it compare in terms of velocity with the Batch Gradient Descent?\n",
    "\n",
    "Try to run the previous 2 cells again. Do you get always the same result?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a linear regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you found the best parameters for your linear model, you can use them on the test data to predict values and evaluate how well your model perform, that is how much your predictions are off from the true y values of your test data.\n",
    "Let's start with the model obtained with LinearRegression() and then do the same with the linear regression obtained via SGD. \n",
    "How do the 2 compare on the test data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete the code to obtain the model prediction for the test data, e.g. diabetes_X_test\n",
    "# for both cases, using the .predict method\n",
    "\n",
    "diabetes_y_pred_EasyLinReg = regr.predict(diabetes_X_test)\n",
    "diabetes_y_pred_SGD = sgd_reg.predict(diabetes_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression() Mean squared error: 3327.59\n",
      "SGD() Mean squared error: 3345.39\n"
     ]
    }
   ],
   "source": [
    "print('LinearRegression() Mean squared error: %.2f' % mean_squared_error(diabetes_y_test, diabetes_y_pred_EasyLinReg))\n",
    "print('SGD() Mean squared error: %.2f' % mean_squared_error(diabetes_y_test, diabetes_y_pred_SGD))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
